{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from collections import defaultdict \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch\n",
    "from  torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = './flickr8/Images'\n",
    "CAPTIONS_PATH = './flickr8/captions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, captions_path, images_path, transform=transforms.ToTensor()):\n",
    "        self.tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "        self.img_caption_dict = self._load_img_caption_dict(captions_path)\n",
    "        self.caption_vocab = self._get_vocab(self.img_caption_dict)\n",
    "        self.image_paths = [f\"{images_path}/{img_name}\" for img_name in self.img_caption_dict.keys()]\n",
    "        self.image_captions = list(self.img_caption_dict.values())\n",
    "        self.transform = transform\n",
    "        \n",
    "    def _get_vocab(self, img_caption_dict):\n",
    "        counter = Counter()\n",
    "        for img_key in img_caption_dict:\n",
    "            for caption in img_caption_dict[img_key]:\n",
    "                counter.update(self.tokenizer(caption))\n",
    "        caption_vocab = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "        caption_vocab.lookup_token(100)\n",
    "        caption_vocab.set_default_index(caption_vocab[\"<unk>\"])\n",
    "        return caption_vocab\n",
    "    \n",
    "    def _load_img_caption_dict(self, captions_path):\n",
    "        img_capt_dict = defaultdict(list)\n",
    "        with open(captions_path, 'r') as captions_file:\n",
    "            for line in captions_file.readlines():\n",
    "                if line.startswith(\"image\"):\n",
    "                    # header\n",
    "                    continue\n",
    "                \n",
    "                else:\n",
    "                    current_line = line.split(',')\n",
    "                    img = current_line[0]\n",
    "                    capt = current_line[1]\n",
    "                    img_capt_dict[img].append(capt)\n",
    "                    \n",
    "        return img_capt_dict        \n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_caption_dict)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths[index]).convert(\"RGB\")\n",
    "        image_tensor = self.transform(image)\n",
    "        \n",
    "        data = []\n",
    "        for caption in self.image_captions[index]:\n",
    "            tokens = self.tokenizer(caption)\n",
    "            tensor = torch.cat([\n",
    "                torch.tensor([self.caption_vocab['<bos>']]),\n",
    "                torch.tensor([self.caption_vocab[token] for token in tokens]),\n",
    "                torch.tensor([self.caption_vocab['<eos>']])\n",
    "            ])\n",
    "            data.append(tensor)\n",
    "                \n",
    "        \n",
    "        return image_tensor, pad_sequence(data, padding_value=self.caption_vocab['<pad>'])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FlickrDataset(CAPTIONS_PATH, IMAGES_PATH, transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((356, 356)),\n",
    "            transforms.RandomCrop((299, 299)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19, 5])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(9)[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        imgs = torch.stack([item[0] for item in batch], dim=0)\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n",
    "\n",
    "        return imgs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=Collate(pad_idx=dataset.caption_vocab['<pad>']),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/image_captioning/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, train=False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.use_pretrained = not train\n",
    "        self.inception = models.inception_v3(pretrained=self.use_pretrained, aux_logits=True) # TODO - aux_logits=False giving issues\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
    "        # make sure output of cnn model is embed size\n",
    "        \n",
    "        for name, param in self.inception.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = train\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "                \n",
    "        \n",
    "        # self.main = nn.Sequential(\n",
    "        #     inception,\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.5)\n",
    "        # )\n",
    "\n",
    "    def forward(self, images):\n",
    "        inception_output = self.inception(images)\n",
    "        return self.dropout(self.relu(inception_output[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        print(embeddings.size())\n",
    "        print(features.unsqueeze(0).size())\n",
    "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        print(features.size())\n",
    "        print(captions.size())\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def caption_image(self, image, vocabulary, max_length=50):\n",
    "        result_caption = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoded_image = self.encoder(image).unsqueeze(0)\n",
    "            states = None\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                hiddens, states = self.decoder.lstm(encoded_image, states)\n",
    "                output = self.decoder.linear(hiddens.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "                predicted_word = vocabulary[predicted.item()]\n",
    "                result_caption.append(predicted_word)\n",
    "                encoded_image = self.decoder.embed(predicted).unsqueeze(0)\n",
    "\n",
    "                if predicted_word == \"<eos>\":\n",
    "                    break\n",
    "\n",
    "        return result_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNtoRNN(embed_size=256, hidden_size=256, vocab_size=len(dataset.caption_vocab), num_layers=1).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.caption_vocab['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(2):\n",
    "    for idx, (imgs, captions) in enumerate(loader):\n",
    "        imgs = imgs.to(device)\n",
    "        captions = captions.to(device)\n",
    "        outputs = model(imgs, captions[:-1])\n",
    "        print(f\"output size: {outputs.size()}\")\n",
    "        loss = criterion(\n",
    "            outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1)\n",
    "        )\n",
    "        print(f\"loss: {loss}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(loss)\n",
    "        optimizer.step()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
